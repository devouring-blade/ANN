# gradient descent optimizer  
gradient descent is the most basic optimizer and has the advantage of being easy to understand and implement. however, this has some 
disadvantage, such as being easily trapped in local minima or having an oscillating path to the target point.

<img width="1876" height="676" alt="{E76904C2-EBAA-4379-A34F-3016117188CF}" src="https://github.com/user-attachments/assets/da50a87f-fbde-4102-8171-3b1f64c40535" />


