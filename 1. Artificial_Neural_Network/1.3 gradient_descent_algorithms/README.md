# gradient descent
<img width="1609" height="759" alt="{286A9E8A-E0F5-4A5A-81B8-344A35EECDD0}" src="https://github.com/user-attachments/assets/f8ce78ac-62a3-4cb8-ace7-28dbdcc53a38" />

# loss surface and local minimum problem
This is a simplified example illustrating the behavior of the loss function when varying only two weights in a simple neural network. In reality, the variation of the loss with respect to all weights is represented in a high-dimensional space, whose landscape is significantly more complex.

<img width="1570" height="750" alt="{536BFAB6-F3A6-4342-A15F-6ED39FD56472}" src="https://github.com/user-attachments/assets/b5a86fc8-ae77-4edd-b523-75e88b772c0c" />

Similar to machine learning in general, artificial neural networks also suffer from the issue of local minima. This problem has not been fundamentally solved; however, methods to further mitigate it are still being actively developed. We will explore this topic in more detail in a later section.

