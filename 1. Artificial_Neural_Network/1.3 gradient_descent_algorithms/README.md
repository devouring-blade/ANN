# gradient descent
<img width="1609" height="759" alt="{286A9E8A-E0F5-4A5A-81B8-344A35EECDD0}" src="https://github.com/user-attachments/assets/f8ce78ac-62a3-4cb8-ace7-28dbdcc53a38" />

# loss surface and local minimum problem
This is a simplified example illustrating the behavior of the loss function when varying only two weights in a simple neural network. In reality, the variation of the loss with respect to all weights is represented in a high-dimensional space, whose landscape is significantly more complex.

<img width="1570" height="750" alt="{536BFAB6-F3A6-4342-A15F-6ED39FD56472}" src="https://github.com/user-attachments/assets/b5a86fc8-ae77-4edd-b523-75e88b772c0c" />

Similar to machine learning in general, artificial neural networks also suffer from the issue of local minima. This problem has not been fundamentally solved; however, methods to further mitigate it are still being actively developed. We will explore this topic in more detail in a later section.

# finding gradients
<img width="1576" height="749" alt="{DC067FA5-5ED5-44EA-B3E4-0A5D135AC274}" src="https://github.com/user-attachments/assets/2c845c28-629c-455e-9442-7c8a6fc37133" />

# finding approximate gradients of activation functions using numerical differentiation
<img width="1572" height="730" alt="{A8D249FD-4AE4-4321-87BB-3EF5675E8265}" src="https://github.com/user-attachments/assets/1e9973e3-7738-4d3c-92c0-910e6eb8d0d8" />




