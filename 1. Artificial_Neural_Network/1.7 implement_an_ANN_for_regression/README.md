# implement an ANN using numerical differentiation and gradient descent for regression
linear regression can be implemented using a single-layer Perceptron (SLP), and non-linear regression can be implemented using a 
multi-layer Perceptron (MLP)
The output layer uses a linear activation function (f(z) = z), and the hidden layer uses nonlinear activation function, such as sigmoid, tanh, ReLU.
Mean squared error (MSE) is used as the loss function.

<img width="1311" height="549" alt="{E597754D-FE13-4256-A899-7CF6A5CEE51B}" src="https://github.com/user-attachments/assets/df5b835b-7504-4ac8-b471-05dfc14b0fec" />


